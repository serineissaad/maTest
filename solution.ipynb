{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e694ac",
   "metadata": {},
   "source": [
    "## 0. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd6f3b",
   "metadata": {},
   "source": [
    "This benchmark is created with the use of an LLm. After some quick data exploration, it's safe to say the prompt has a unique value(same prompt) and that the columns, aside from image and cadquery, are metadata.\n",
    "\n",
    "This suggested solution uses the pix2struct-base pre-trained model from google. The model takes an image as input and generates the cadquery or code as output.\n",
    "\n",
    "**Note:** It's important to highlight that after the train/test split, the train data contains no ``hundred_subset=True`` samples, because there's no shuffling before. This *could* impact the results if there's a particular modification in ``True`` samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259d4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset as HFDataset, load_dataset  # Hugging Face Dataset\n",
    "from tqdm import tqdm\n",
    "from metrics.best_iou import get_iou_best\n",
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4794b",
   "metadata": {},
   "source": [
    "## 1. Passing to the real things "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a44c3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CADDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, processor):\n",
    "        self.data = hf_dataset  # Directly use HF Dataset (no pandas needed)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]  # Use direct indexing for HF Dataset\n",
    "        \n",
    "        # Process image\n",
    "        image_encoding = self.processor(\n",
    "            images=item['image'], \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process text (cadquery) for labels\n",
    "        text_encoding = self.processor(\n",
    "            text=item['cadquery'],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Combine the encodings\n",
    "        encoding = {\n",
    "            'flattened_patches': image_encoding['flattened_patches'],\n",
    "            'attention_mask': image_encoding['attention_mask'],\n",
    "            'labels': text_encoding['input_ids']\n",
    "        }\n",
    "        \n",
    "        return {k: v.squeeze() for k, v in encoding.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6159954",
   "metadata": {},
   "source": [
    "### Model Sweet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd91ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-base\")\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-base\").to(device)\n",
    "\n",
    "# 3. Load Data (corrected)\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=16, split=[\"train\", \"test\"], \n",
    "                 cache_dir=\"/Volumes/BIG-DATA/HUGGINGFACE_CACHE\")\n",
    "\n",
    "# 4. Create Datasets\n",
    "train_dataset = ds[0]\n",
    "test_dataset = ds[1]\n",
    "\n",
    "train_data = train_dataset.select(range(len(train_dataset) // 4 ))\n",
    "test_data = test_dataset.select(range(len(test_dataset) // 4))\n",
    "\n",
    "train_data=CADDataset(train_data, processor)\n",
    "# test_data=CADDataset(test_data, processor)\n",
    "\n",
    "# 5. Training Setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbf4bf",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ca50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/9206 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 19.7514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 1/9206 [02:06<323:50:24, 126.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 15.6811\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 1/9206 [04:11<642:57:54, 251.46s/it]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item():.4f}\\n\")\n",
    "        i+=1\n",
    "\n",
    "        model.save_pretrained(\"pix2struct-cad-model-light\")\n",
    "        processor.save_pretrained(\"pix2struct-cad-model-light\")\n",
    "\n",
    "        if i== 2:\n",
    "            break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dae431",
   "metadata": {},
   "source": [
    "### Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b963b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|██▏       | 22/100 [48:19<5:01:46, 232.14s/it]"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n",
    "# from datasets import Dataset as HFDataset\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load saved model and processor\n",
    "model_path = \"pix2struct-cad-model-light\"\n",
    "\n",
    "processor = Pix2StructProcessor.from_pretrained(model_path)\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 2. Define generation function\n",
    "def generate_code(model, processor, image, max_length=512):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=max_length)\n",
    "    return processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 3. Evaluation function\n",
    "def evaluate_model(model, test_data, processor, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        num_samples = len(test_data)\n",
    "    \n",
    "    generated_codes = []\n",
    "    target_codes = []\n",
    "    \n",
    "    for i in tqdm(range(num_samples), desc=\"Evaluating\"):\n",
    "        sample = test_data[i]\n",
    "        image = sample['image']\n",
    "        target_code = sample['cadquery']\n",
    "        \n",
    "        # Generate prediction\n",
    "        pred_code = generate_code(model, processor, image)\n",
    "        \n",
    "        generated_codes.append(pred_code)\n",
    "        target_codes.append(target_code)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    vsr = evaluate_syntax_rate_simple(generated_codes)\n",
    "    iou_scores = [get_iou_best(target, pred) for target, pred in zip(target_codes, generated_codes)]\n",
    "    avg_iou = np.mean(iou_scores)\n",
    "    \n",
    "    return {\n",
    "        \"valid_syntax_rate\": vsr,\n",
    "        \"average_iou\": avg_iou,\n",
    "        \"generated_examples\": list(zip(target_codes, generated_codes))[:5]  # First 5 samples\n",
    "    }\n",
    "\n",
    "\n",
    "# 5. Run evaluation\n",
    "results = evaluate_model(model, test_data, processor, num_samples=100)  # Evaluate on 100 samples\n",
    "\n",
    "# 6. Print results\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Valid Syntax Rate: {results['valid_syntax_rate']:.4f}\")\n",
    "print(f\"Average IOU Score: {results['average_iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae6c5f",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3f897",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, we saved our model in ``/pix2struct-cad-model-light`` during first epoch after 2nd batch. Why? Bacause my pc would stop everytime.. Tried to minimize the dataset to 1/4 but it still didn't work. After all, we're dealing with images and transformers in a cpu.\n",
    "The evalution, for some reason, had to take a long time too and reached 22% by the time this repo will be pushed. We might push another commit later on once finished.\n",
    "To recap, we couldn't analyse any results ==> no code improved, but the idea is there.\n",
    "\n",
    "**Enhacements:**\n",
    "- Use a **VM** for training.. I actually laughed when I remembered \"Enhance by any manner the baseline model and evaluate it again\" from the instructions.. I got too lucky to see the **blue screen** twice today :'')\n",
    "- Fine-tune hyperparameters: learning rate, batch size, and number of epochs.\n",
    "- We could've used another metric, like BLEU\n",
    "- use pix2struct-large for better performence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
